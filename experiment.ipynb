{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "808487c24e26b33d",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "This notebook reproduces the results presented in *nanoTabPFN: A Lightweight and Educational Reimplementation of TabPFN*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add47b5e197fe73f",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3ffc8a5b719b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import functools\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import openml\n",
    "import pandas as pd\n",
    "from openml.tasks import TaskType\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder, OrdinalEncoder, FunctionTransformer\n",
    "\n",
    "\"\"\"\n",
    "=================== DATA LOADING AND PREPROCESSING ===================\n",
    "\"\"\"\n",
    "\n",
    "def get_feature_preprocessor(X: np.ndarray | pd.DataFrame) -> ColumnTransformer:\n",
    "    \"\"\"\n",
    "    fits a preprocessor that imputes NaNs, encodes categorical features and removes constant features\n",
    "    \"\"\"\n",
    "    X = pd.DataFrame(X)\n",
    "    num_mask = []\n",
    "    cat_mask = []\n",
    "    for col in X:\n",
    "        unique_non_nan_entries = X[col].dropna().unique()\n",
    "        if len(unique_non_nan_entries) <= 1:\n",
    "            num_mask.append(False)\n",
    "            cat_mask.append(False)\n",
    "            continue\n",
    "        non_nan_entries = X[col].notna().sum()\n",
    "        numeric_entries = pd.to_numeric(X[col], errors='coerce').notna().sum() # in case numeric columns are stored as strings\n",
    "        num_mask.append(non_nan_entries == numeric_entries)\n",
    "        cat_mask.append(non_nan_entries != numeric_entries)\n",
    "        # num_mask.append(is_numeric_dtype(X[col]))  # Assumes pandas dtype is correct\n",
    "\n",
    "    num_mask = np.array(num_mask)\n",
    "    cat_mask = np.array(cat_mask)\n",
    "\n",
    "    num_transformer = Pipeline([\n",
    "        (\"to_pandas\", FunctionTransformer(lambda x: pd.DataFrame(x) if not isinstance(x, pd.DataFrame) else x)), # to apply pd.to_numeric of pandas\n",
    "        (\"to_numeric\", FunctionTransformer(lambda x: x.apply(pd.to_numeric, errors='coerce').to_numpy())), # in case numeric columns are stored as strings\n",
    "    ])\n",
    "    cat_transformer = Pipeline([\n",
    "        ('encoder', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=np.nan)),\n",
    "    ])\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', num_transformer, num_mask),\n",
    "            ('cat', cat_transformer, cat_mask)\n",
    "        ]\n",
    "    )\n",
    "    return preprocessor\n",
    "\n",
    "def get_openml_datasets(\n",
    "        max_features_eval: int = 10, \n",
    "        new_instances_eval: int = 200, \n",
    "        target_classes_filter: int = 2,\n",
    "        **kwargs,\n",
    "        ) -> dict[str, tuple[np.ndarray, np.ndarray]]:\n",
    "    \"\"\"\n",
    "    Load OpenML tabarena datasets with at most `max_features` features and subsampled (stratified) to `new_instances` instances.\n",
    "    \"\"\"\n",
    "    task_ids = [\n",
    "        363612, 363613, 363614, 363615, 363616, 363618, 363619, 363620,\n",
    "        363621, 363623, 363624, 363625, 363626, 363627, 363628, 363629,\n",
    "        363630, 363631, 363632, 363671, 363672, 363673, 363674, 363675,\n",
    "        363676, 363677, 363678, 363679, 363681, 363682, 363683, 363684,\n",
    "        363685, 363686, 363689, 363691, 363693, 363694, 363696, 363697,\n",
    "        363698, 363699, 363700, 363702, 363704, 363705, 363706, 363707,\n",
    "        363708, 363711, 363712\n",
    "    ] # TabArena v0.1\n",
    "    datasets = {}\n",
    "    for task_id in task_ids:\n",
    "        task = openml.tasks.get_task(task_id, download_splits=False)\n",
    "        if task.task_type_id != TaskType.SUPERVISED_CLASSIFICATION:\n",
    "            continue  # skip task, only classification\n",
    "        dataset = task.get_dataset(download_data=False)\n",
    "\n",
    "        if dataset.qualities[\"NumberOfFeatures\"] > max_features_eval or (dataset.qualities[\"NumberOfClasses\"] > target_classes_filter) or dataset.qualities[\"PercentageOfInstancesWithMissingValues\"] > 0 or dataset.qualities[\"MinorityClassPercentage\"] < 2.5:\n",
    "            continue\n",
    "        X, y, categorical_indicator, attribute_names = dataset.get_data(\n",
    "            target=task.target_name, dataset_format=\"dataframe\"\n",
    "        )\n",
    "        if new_instances_eval < len(y):\n",
    "            _, X_sub, _, y_sub = train_test_split(\n",
    "                X, y,\n",
    "                test_size=new_instances_eval,\n",
    "                stratify=y,\n",
    "                random_state=0,\n",
    "            )\n",
    "        else:\n",
    "            X_sub = X\n",
    "            y_sub = y\n",
    "        \n",
    "        X = X_sub.to_numpy(copy=True)\n",
    "        y = y_sub.to_numpy(copy=True)\n",
    "        label_encoder = LabelEncoder()\n",
    "        y = label_encoder.fit_transform(y)\n",
    "\n",
    "        preprocessor = get_feature_preprocessor(X)\n",
    "        X = preprocessor.fit_transform(X)\n",
    "        datasets[dataset.name] = (X, y)\n",
    "    return datasets\n",
    "\n",
    "\"\"\"\n",
    "=================== EVALUATION ===================\n",
    "\"\"\"\n",
    "\n",
    "_skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "\n",
    "def eval_model(model, datasets):\n",
    "    \"\"\"Evaluates a model on multiple datasets and returns metrics\"\"\"\n",
    "    metrics = {}\n",
    "    for dataset_name, (X,y)  in datasets.items():\n",
    "        targets = []\n",
    "        probabilities = []\n",
    "        \n",
    "        for train_idx, test_idx in _skf.split(X, y):\n",
    "            X_train, X_test = X[train_idx], X[test_idx]\n",
    "            y_train, y_test  = y[train_idx], y[test_idx]\n",
    "            targets.append(y_test)\n",
    "            model.fit(X_train, y_train)\n",
    "            y_proba = model.predict_proba(X_test)\n",
    "            if y_proba.shape[1] == 2:  # binary classification with neural network\n",
    "                y_proba = y_proba[:, 1]\n",
    "            probabilities.append(y_proba)\n",
    "    \n",
    "        targets = np.concatenate(targets, axis=0)\n",
    "        probabilities = np.concatenate(probabilities, axis=0)\n",
    "\n",
    "        metrics[f\"{dataset_name}/ROC AUC\"] = roc_auc_score(targets, probabilities, multi_class=\"ovr\")\n",
    "    \n",
    "    metric_names = list({key.split(\"/\")[-1] for key in metrics.keys()})\n",
    "    for metric_name in metric_names:\n",
    "        avg_metric = np.mean([metrics[key] for key in metrics.keys() if key.endswith(metric_name)])\n",
    "        metrics[f\"{metric_name}\"] = float(avg_metric)\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\"\"\"\n",
    "=================== PLOTTING ===================\n",
    "\"\"\"\n",
    "\n",
    "def plot_runs(\n",
    "        ax: plt.Axes, \n",
    "        runs: list[pd.DataFrame], \n",
    "        metric: str, \n",
    "        baselines: pd.DataFrame = None,\n",
    "        baselines_std: pd.DataFrame = None,\n",
    "        show_legend: bool = True,\n",
    "        show_xlabel: bool = True,\n",
    "        show_ylabel: bool = True,\n",
    "        show_xtics: bool = True\n",
    "        ):\n",
    "    \"\"\"\n",
    "    Plots the run for a given metric and adds baselines\n",
    "\n",
    "    `runs` is a list of dataframes where each dataframe corresponds \n",
    "    to a run from a model with the same config but a different seed.\n",
    "    Each dataframe needs to have a `\"training_time\"` column and a `metric`column.\n",
    "\n",
    "    `baselines` is a DataFrame with metric columns and rows whose index correspond to\n",
    "    a ML algorithm.\n",
    "    \"\"\"\n",
    "    colors = sns.color_palette(\"tab10\")[1:]\n",
    "    linestyles = [\n",
    "        '--',      # dashed\n",
    "        '-.',      # dash-dot\n",
    "        ':',       # dotted\n",
    "        (0, (3, 1, 1, 1)),  # dash-dot-dot\n",
    "        (0, (5, 5))         # spaced dash\n",
    "    ]\n",
    "\n",
    "    training_times = [run[\"training_time\"].tolist() for run in runs]\n",
    "    training_times = sorted(set([item for sublist in training_times for item in sublist]))\n",
    "    shared_time_runs = []\n",
    "    for run in runs:\n",
    "        run = run.copy()\n",
    "        run = run[[metric, \"training_time\"]].set_index(\"training_time\").reindex(training_times)\n",
    "        run = run.interpolate()\n",
    "        shared_time_runs.append(run)\n",
    "    all_runs = pd.concat(shared_time_runs, axis=1).dropna()\n",
    "    \n",
    "    # plot mean and std of all runs or single run if only one run\n",
    "    mean = all_runs.mean(axis=1)\n",
    "    ax.plot(mean.index, mean, label=\"nanoTabPFN\", zorder=2, color=\"blue\")\n",
    "    if all_runs.shape[1] > 1: # more than one run\n",
    "        std = all_runs.std(axis=1)\n",
    "        ax.fill_between(mean.index, mean - std, mean + std, alpha=0.2, zorder=2)\n",
    "\n",
    "    # plot horizontal lines for baselines\n",
    "    if baselines is not None:\n",
    "        for i ,(baseline_name, baseline_value) in enumerate(baselines[metric].items()):\n",
    "            # draw a horizontal line that ends at the same x as the runs\n",
    "            color = colors[i % len(colors)]\n",
    "            ax.plot([0, max(training_times)], [baseline_value, baseline_value], label=baseline_name, alpha=0.7, linestyle=linestyles[i], color=color, zorder=1)\n",
    "            if baselines_std is not None and baseline_name in baselines_std.index:\n",
    "                    std = baselines_std.loc[baseline_name, metric]\n",
    "                    ax.fill_between([0, max(training_times)], [baseline_value - std, baseline_value - std], [baseline_value + std, baseline_value + std], alpha=0.2, zorder=1)\n",
    "    \n",
    "    # Plot Style\n",
    "    ax.grid(True, axis=\"y\")\n",
    "    ax.grid(False, axis=\"x\")\n",
    "    ax.tick_params(axis=\"y\", length=0)\n",
    "    if not show_xtics:\n",
    "        ax.tick_params(axis=\"x\", length=0)\n",
    "    if show_xlabel:\n",
    "        ax.set_xlabel(\"Training time (seconds)\")\n",
    "    if show_ylabel:\n",
    "        ax.set_ylabel(metric.split(\"/\")[-1])\n",
    "    max_time = max(training_times)\n",
    "    ax.set_xlim(0, max_time)\n",
    "    ylim = ax.get_ylim()\n",
    "    ax.set_ylim(ylim[0], 1)\n",
    "    \n",
    "    # order legend entries by their y-value at the end of the plot\n",
    "    if show_legend:\n",
    "        handles, labels = ax.get_legend_handles_labels()\n",
    "        label_y_values = {}\n",
    "        for handle, label in zip(handles, labels):\n",
    "            if isinstance(handle, plt.Line2D):\n",
    "                y_data = handle.get_ydata()\n",
    "                label_y_values[label] = y_data[-1]\n",
    "        sorted_labels = sorted(label_y_values.items(), key=lambda x: x[1], reverse=True)\n",
    "        sorted_handles = [handle for label, _ in sorted_labels for handle, lbl in zip(handles, labels) if lbl == label]\n",
    "        sorted_labels = [label for label, _ in sorted_labels]\n",
    "        ax.legend(sorted_handles, sorted_labels)\n",
    "        \n",
    "    # remove border\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_visible(False)\n",
    "\n",
    "def plot_run_grid(runs: list[pd.DataFrame], baselines: pd.DataFrame = None, baselines_std: pd.DataFrame = None):\n",
    "    \"\"\"Plots the runs in a grid of metrics x datasets\"\"\"\n",
    "    # drop all columns without \"/\" for dataset/metric format\n",
    "    datasets = list(set([col.split(\"/\")[0] for col in runs[0].columns if \"/\" in col]))\n",
    "    metric = 'ROC AUC'\n",
    "    figsize = (len(datasets) * 4, 4.6)\n",
    "    fig, axs = plt.subplots(1, len(datasets), figsize=figsize, sharex=True, sharey=True, layout=\"constrained\")\n",
    "    fig.set_constrained_layout_pads(w_pad=0.0, h_pad=0.1)\n",
    "    # Plot each metric and dataset\n",
    "    for j, dataset in enumerate(datasets):\n",
    "        ax = axs[j]\n",
    "        plot_runs(ax, runs, f\"{dataset}/{metric}\", baselines, baselines_std, show_legend=False, show_xlabel=False, show_ylabel=(j==0))\n",
    "        ax.set_title(dataset)\n",
    "    fig.supxlabel(\"Training Time (seconds)\")\n",
    "    \n",
    "    # y-axis and x-axis labels should have the same size as supxlabel\n",
    "    for ax in axs.flatten():\n",
    "        font_size = fig.texts[-1].get_fontsize() \n",
    "        ax.xaxis.label.set_size(font_size)\n",
    "        ax.yaxis.label.set_size(font_size)\n",
    "    \n",
    "    # Create a single legend for the entire figure\n",
    "    legend_handels_labels = [list(zip(*ax.get_legend_handles_labels())) for ax in axs.flatten()]\n",
    "    legend_handels_labels = functools.reduce(lambda a, b: a + b, legend_handels_labels)\n",
    "    unique = dict([(label, handle) for (handle, label) in legend_handels_labels])\n",
    "    fig.legend(unique.values(), unique.keys(), loc=\"outside upper center\", ncol=3)   \n",
    "    return fig, axs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405d417923531ee4",
   "metadata": {},
   "source": [
    "# Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150287b1bb276cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SEEDS = 2\n",
    "# NUM_SEEDS = 20 # If you want to reproduce the paper results, use 20 seeds\n",
    "DATASETS = get_openml_datasets(max_features=10, new_instances=200, target_classes_filter=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe44821996092fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from tabpfn import TabPFNClassifier\n",
    "from tabpfn.config import ModelInterfaceConfig, PreprocessorConfig\n",
    "\n",
    "\n",
    "no_preprocessing_inference_config = ModelInterfaceConfig(\n",
    "    FINGERPRINT_FEATURE=False,\n",
    "    PREPROCESS_TRANSFORMS=[PreprocessorConfig(name='none')]\n",
    ")\n",
    "\n",
    "baseline_models = {\n",
    "    \"TabPFN v2\": [TabPFNClassifier(random_state=i) for i in range(NUM_SEEDS)],\n",
    "    \"TabPFN v2 (no preprocessing)\": [TabPFNClassifier(inference_config=no_preprocessing_inference_config, n_estimators=1, random_state=i) for i in range(NUM_SEEDS)],\n",
    "    \"Random Forest\": [RandomForestClassifier(random_state=i) for i in range(NUM_SEEDS)],\n",
    "    \"K-Nearest Neighbors\": [KNeighborsClassifier()],\n",
    "    \"Decision Tree\": [DecisionTreeClassifier(random_state=i) for i in range(NUM_SEEDS)],\n",
    "}\n",
    "\n",
    "baseline_models_eval = {name: [eval_model(model, datasets=DATASETS) for model in models] for name, models in baseline_models.items()}\n",
    "\n",
    "def apply_aggregation(eval_results: dict, func=np.mean):\n",
    "    aggregated_result = {}\n",
    "    for result in eval_results:\n",
    "        for metric, value in result.items():\n",
    "            if metric not in aggregated_result:\n",
    "                aggregated_result[metric] = []\n",
    "            aggregated_result[metric].append(value)\n",
    "    for metric in aggregated_result:\n",
    "        aggregated_result[metric] = func(aggregated_result[metric])\n",
    "    return aggregated_result\n",
    "\n",
    "baselines = pd.DataFrame({\n",
    "    name: apply_aggregation(models, np.mean) for name, models in baseline_models_eval.items()\n",
    "}).T\n",
    "\n",
    "baselines_std = pd.DataFrame({\n",
    "    name: apply_aggregation(models, np.std) for name, models in baseline_models_eval.items()\n",
    "}).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba43db5398b5f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import NanoTabPFNModel\n",
    "from train import PriorDumpDataLoader, train, get_default_device, set_randomness_seed\n",
    "\n",
    "histories = []\n",
    "device = get_default_device()\n",
    "\n",
    "for seed in range(NUM_SEEDS):\n",
    "    set_randomness_seed(seed)\n",
    "    prior = PriorDumpDataLoader(\"300k_150x5_2.h5\", num_steps=2500, batch_size=32, device=device)\n",
    "    model = NanoTabPFNModel(\n",
    "            embedding_size=96,\n",
    "            num_attention_heads=4,\n",
    "            mlp_hidden_size=192,\n",
    "            num_layers=3,\n",
    "            num_outputs=2\n",
    "        )\n",
    "    eval_model_partial = functools.partial(eval_model, datasets=DATASETS)\n",
    "    model, history = train(model, prior, lr=4e-3, steps_per_eval=25, eval_func=eval_model_partial)\n",
    "    history = [{**record, \"training_time\": training_time} for (training_time, record) in history]\n",
    "    history = pd.DataFrame(history)\n",
    "    histories.append(history)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bedb8b195ca9f21",
   "metadata": {},
   "source": [
    "# Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218270a36414c4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = \"ROC AUC\"\n",
    "sns.set_style(\"ticks\")\n",
    "sns.set_context(\"notebook\")\n",
    "fig, ax = plt.subplots(figsize=(6,4.5), layout=\"constrained\")\n",
    "plot_runs(ax, histories, metric, baselines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bda71d19cb9bbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context(\"notebook\")\n",
    "fig, ax = plot_run_grid(histories, baselines)\n",
    "plt.show()\n",
    "fig.savefig(f\"nanotabpfn_summary.png\", dpi=300)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
